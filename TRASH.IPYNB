{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (4) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [-1, 4, -1, -1].  Tensor sizes: [2, 1, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, seq_len)\u001b[38;5;241m.\u001b[39mbool()  \u001b[38;5;66;03m# Giả sử không có padding\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Thêm một chiều và mở rộng mask để phù hợp với số đầu (heads)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Kích thước: [batch_size, num_heads, seq_len, seq_len]\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Tạo một dropout module\u001b[39;00m\n\u001b[1;32m     22\u001b[0m dropout \u001b[38;5;241m=\u001b[39m Dropout(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (4) must match the existing size (2) at non-singleton dimension 1.  Target sizes: [-1, 4, -1, -1].  Tensor sizes: [2, 1, 10]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model  # Embedding vector size\n",
    "        self.h = h  # Number of heads\n",
    "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.d_k = d_model // h  # Dimension of vector seen by each head\n",
    "        self.q_lin = nn.Linear(d_model, d_model, bias=True)  # Wq\n",
    "        self.k_lin = nn.Linear(d_model, d_model, bias=True)  # Wk\n",
    "        self.v_lin = nn.Linear(d_model, d_model, bias=True)  # Wv\n",
    "        self.out_lin = nn.Linear(d_model, d_model, bias=True)  # Wo\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            batch_size, num_heads, seq_len, _ = attention_scores.size()\n",
    "            mask = mask.unsqueeze(1)  # Chỉ cần một lần unsqueeze ở đây\n",
    "            mask = mask.expand(batch_size, num_heads, seq_len, seq_len)\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        return (attention_scores @ value), attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.q_lin(q)\n",
    "        key = self.k_lin(k)\n",
    "        value = self.v_lin(v)\n",
    "\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.d_model)\n",
    "        return self.out_lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([10, 64])\n",
      "Attention Scores shape: torch.Size([10, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Dropout\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.q_lin = nn.Linear(d_model, d_model)\n",
    "        self.k_lin = nn.Linear(d_model, d_model)\n",
    "        self.v_lin = nn.Linear(d_model, d_model)\n",
    "        self.out_lin = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def attention(self, query, key, value, mask, dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        # Compute attention scores\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        # Multiply scores with value\n",
    "        output = attention_scores @ value\n",
    "        return output, attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        batch_size = q.size(0)\n",
    "        # Apply linear projection and split into h heads\n",
    "        q = self.q_lin(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        k = self.k_lin(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        v = self.v_lin(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # Calculate attention\n",
    "        outputs, scores = self.attention(q, k, v, mask, self.dropout)\n",
    "        # Concatenate heads and put through final linear layer\n",
    "        outputs = outputs.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out_lin(outputs)\n",
    "\n",
    "# Parameters\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "d_model = 64  # Embedding size\n",
    "num_heads = 4\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create query, key, value\n",
    "query = torch.rand(batch_size, seq_len, d_model)\n",
    "key = torch.rand(batch_size, seq_len, d_model)\n",
    "value = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create mask\n",
    "mask = torch.zeros(batch_size, seq_len).bool()  # Assuming no padding\n",
    "mask = mask.unsqueeze(1).unsqueeze(2).expand(-1, num_heads, -1, -1)  # Dimension: [batch_size, num_heads, seq_len, seq_len]\n",
    "\n",
    "# Create dropout module\n",
    "dropout = Dropout(p=dropout_rate)\n",
    "\n",
    "# Initialize MultiHeadAttention module\n",
    "mha = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "\n",
    "# Call attention method\n",
    "output, attention_scores = mha(query, key, value, mask)\n",
    "print(\"Output shape:\", output.shape)  # Expected shape: [batch_size, seq_len, d_model]\n",
    "print(\"Attention Scores shape:\", attention_scores.shape)  # Expected shape: [batch_size, num_heads, seq_len, seq_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.q_lin = nn.Linear(d_model, d_model)\n",
    "        self.k_lin = nn.Linear(d_model, d_model)\n",
    "        self.v_lin = nn.Linear(d_model, d_model)\n",
    "        self.out_lin = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def attention(self, query, key, value, mask, dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        attention_scores = F.softmax(attention_scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        output = attention_scores @ value\n",
    "        return output, attention_scores\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        batch_size = q.size(0)\n",
    "        q = self.q_lin(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        k = self.k_lin(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        v = self.v_lin(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # Adjust mask for multi-head\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand(batch_size, self.h, -1, -1)  # Adjust mask shape for multi-head attention\n",
    "\n",
    "        outputs, scores = self.attention(q, k, v, mask, self.dropout)\n",
    "        outputs = outputs.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        return self.out_lin(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 60, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Dropout\n",
    "import math\n",
    "\n",
    "# Define MultiHeadAttention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.q_lin = nn.Linear(d_model, d_model)\n",
    "        self.k_lin = nn.Linear(d_model, d_model)\n",
    "        self.v_lin = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.out_lin = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def attention(self, query, key, value, mask):\n",
    "        # Scale the dot product by the dimensions of the key\n",
    "        scaled_attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.depth)\n",
    "        if mask is not None:\n",
    "            scaled_attention_scores += (mask * -1e9)\n",
    "        attention_weights = F.softmax(scaled_attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        batch_size = q.size(0)\n",
    "        q = self.split_heads(self.q_lin(q), batch_size)\n",
    "        k = self.split_heads(self.k_lin(k), batch_size)\n",
    "        v = self.split_heads(self.v_lin(v), batch_size)\n",
    "\n",
    "        attention_output, _ = self.attention(q, k, v, mask)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.out_lin(attention_output)\n",
    "        return output\n",
    "\n",
    "# Dummy data\n",
    "batch_size = 1\n",
    "seq_len = 60\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "# Initialize model\n",
    "dropout_rate = 0.1\n",
    "model = MultiHeadAttention(d_model, num_heads, dropout_rate)\n",
    "\n",
    "# Create query, key, value\n",
    "query = torch.rand(batch_size, seq_len, d_model)\n",
    "key = torch.rand(batch_size, seq_len, d_model)\n",
    "value = torch.rand(batch_size, seq_len, d_model)\n",
    "\n",
    "# Create mask\n",
    "def create_causal_mask(size):\n",
    "    mask = torch.triu(torch.ones(size, size) * float('-inf'), diagonal=1)\n",
    "    return mask == 0\n",
    "\n",
    "mask = create_causal_mask(seq_len).to(torch.bool)\n",
    "mask = mask.unsqueeze(0).unsqueeze(1)  # fit the mask shape for the attention heads: [batch_size, 1, seq_len, seq_len]\n",
    "\n",
    "# Test the forward pass\n",
    "output = model(query, key, value, mask)\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    sos_idx = tokenizer_tgt.convert_tokens_to_ids('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.convert_tokens_to_ids('[EOS]')\n",
    "\n",
    "    encoder_output = model.encoder(source, source_mask)\n",
    "    decoder_input = torch.tensor([[sos_idx]], dtype=torch.long, device=device)\n",
    "\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        decoder_mask = causal_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "        out = model.decoder(decoder_input, encoder_output, source_mask, decoder_mask)\n",
    "        prob = model.projection(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "\n",
    "        decoder_input = torch.cat(\n",
    "            [decoder_input, torch.tensor([[next_word.item()]], dtype=torch.long, device=device)], dim=1\n",
    "        )\n",
    "\n",
    "        if next_word.item() == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from utils.dataset import TranslationDataset\n",
    "\n",
    "datasets = load_dataset(\n",
    "    'csv', \n",
    "    data_files={\n",
    "        'train': '/home/chaos/Documents/ChaosAIVision/dataset/viet2eng/train_dataset.csv',\n",
    "        'validation': '/home/chaos/Documents/ChaosAIVision/dataset/viet2eng/validation_dataset.csv'\n",
    "    }\n",
    ")\n",
    "train_dataset = datasets['train']\n",
    "valid_dataset = datasets['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Giả sử bạn sử dụng tokenizer của BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Tạo dataset\n",
    "train_data = TranslationDataset(train_dataset, tokenizer)\n",
    "valid_data = TranslationDataset(valid_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads        \n",
    "        self.q_lin = nn.Linear(d_model, d_model)\n",
    "        self.k_lin = nn.Linear(d_model, d_model)\n",
    "        self.v_lin = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.out_lin = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def attention(self, query, key, value, mask):\n",
    "        # Scale the dot product by the dimensions of the key\n",
    "        scaled_attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.depth)\n",
    "        if mask is not None:\n",
    "            scaled_attention_scores += (mask * -1e9)\n",
    "        attention_weights = F.softmax(scaled_attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "        return output, attention_weights\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        batch_size = q.size(0)\n",
    "        q = self.split_heads(self.q_lin(q), batch_size)\n",
    "        k = self.split_heads(self.k_lin(k), batch_size)\n",
    "        v = self.split_heads(self.v_lin(v), batch_size)\n",
    "\n",
    "        attention_output, attention_weights = self.attention(q, k, v, mask)\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.out_lin(attention_output)\n",
    "        return output, attention_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim, dropout):\n",
    "        super(FFN, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lin1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.lin2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.sa_layer_norm = nn.LayerNorm(d_model, eps=1e-12)  # Chuẩn hóa lớp Self-Attention\n",
    "        self.ffn = FFN(d_model, ff_dim, dropout)\n",
    "        self.output_layer_norm = nn.LayerNorm(d_model, eps=1e-12)  # Chuẩn hóa lớp đầu ra\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Self-Attention và residual connection\n",
    "        attn_output, _ = self.attention(x, x, x, mask)  # (batch_size, seq_len, d_model)\n",
    "        x = self.sa_layer_norm(x + attn_output)  # Residual connection và chuẩn hóa lớp\n",
    "\n",
    "        # Mạng Feed-Forward và residual connection\n",
    "        ffn_output = self.ffn(x)  # (batch_size, seq_len, d_model)\n",
    "        x = self.output_layer_norm(x + ffn_output)  # Residual connection và chuẩn hóa lớp\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_length, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
    "        self.position_embeddings = nn.Embedding(num_embeddings=max_length, embedding_dim=d_model)\n",
    "        self.Layer_norm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, seq_len = x.size()\n",
    "        positions = torch.arange(0, seq_len, device=self.device).unsqueeze(0)\n",
    "        word_emb = self.word_embeddings(x)\n",
    "        pos_emb = self.position_embeddings(positions)\n",
    "        x = word_emb + pos_emb\n",
    "        return self.Layer_norm(self.dropout(x))\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, d_model, num_heads, ff_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.embd_layer = TokenAndPositionEmbedding(vocab_size, d_model, max_length, dropout, device)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, ff_dim, dropout)\n",
    "            for _ in range(6)  # Number of transformer layers\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embd_layer(x)\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, d_model, num_heads, ff_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.embd_layer = TokenAndPositionEmbedding(vocab_size, d_model, max_length, dropout, device)\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, ff_dim, dropout)\n",
    "            for _ in range(3)  # Number of transformer layers\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.embd_layer(x)\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x, tgt_mask)\n",
    "            # Add cross attention layer\n",
    "            cross_attn_output, _ = self.attention(x, enc_output, enc_output, src_mask)\n",
    "            x = x + cross_attn_output  # Residual connection after cross attention\n",
    "            x = self.layer_norm(x)  # Apply layer normalization after residual connection\n",
    "        return x  # Trả về kích thước (batch_size, seq_len, d_model)\n",
    "\n",
    "\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, max_length, d_model, num_heads, ff_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(vocab_size, max_length, d_model, num_heads, ff_dim, dropout, device)\n",
    "        self.decoder = TransformerDecoder(vocab_size, max_length, d_model, num_heads, ff_dim, dropout, device)\n",
    "        self.projection = nn.Linear(d_model, vocab_size)  # Projection layer\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        enc_output = self.encoder(src, src_mask)\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)\n",
    "        return self.projection(dec_output)  # Dùng lớp projection để chuyển đổi từ d_model -> vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output size: torch.Size([1, 128, 768])\n",
      "Final output size: torch.Size([1, 128, 30522])\n",
      "Output size: torch.Size([1, 128, 30522])\n",
      "Output: tensor([[[-0.0514, -0.1942, -0.7749,  ..., -0.9980,  0.0474, -0.4377],\n",
      "         [-0.7259, -0.8567, -0.4070,  ..., -1.0523,  1.1871, -0.7497],\n",
      "         [ 0.4172,  0.4006,  0.4665,  ...,  0.0643, -1.1330,  0.1484],\n",
      "         ...,\n",
      "         [ 0.8848, -1.2318, -0.1485,  ..., -0.1547,  0.5888, -0.9604],\n",
      "         [-0.0530, -0.2978, -0.4419,  ...,  1.0947,  0.1357, -0.0667],\n",
      "         [-0.2117, -0.4139,  0.0339,  ...,  0.2559,  0.5129, -0.6952]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "d_model = 768\n",
    "num_heads = 12\n",
    "ff_dim = 3072\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerSeq2Seq(\n",
    "        vocab_size=vocab_size,\n",
    "        max_length=max_length,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        ff_dim=ff_dim,\n",
    "        dropout=dropout,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "# Các thông tin cần thiết\n",
    "max_length = 128  # Độ dài tối đa của chuỗi\n",
    "vocab_size = 30522  # Giả sử vocab size của bạn là 30522 (ví dụ cho BERT tokenizer)\n",
    "batch_size = 1  # Batch size của bạn, có thể là 1 hoặc hơn tùy ý\n",
    "\n",
    "# Tạo dummy input giống như dataset\n",
    "input_ids = torch.randint(1, vocab_size, (batch_size, max_length))  # Giả định input_ids có các giá trị ngẫu nhiên từ 1 đến vocab_size\n",
    "attention_mask = torch.ones((batch_size, 1, 1, max_length))  # Giả định tất cả các từ đều được chú ý (1s)\n",
    "decoder_input = torch.randint(1, vocab_size, (batch_size, max_length))  # Giả định input_ids cho decoder\n",
    "decoder_mask = torch.ones((batch_size, 1, max_length, max_length))  # Giả định mask cho decoder\n",
    "labels = torch.randint(1, vocab_size, (batch_size, max_length))  # Giả định nhãn (labels) có các giá trị ngẫu nhiên từ 1 đến vocab_size\n",
    "\n",
    "# Đưa dummy input vào model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "input_ids = input_ids.to(device)\n",
    "attention_mask = attention_mask.to(device)\n",
    "decoder_input = decoder_input.to(device)\n",
    "decoder_mask = decoder_mask.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Đặt mô hình vào chế độ đánh giá\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    encoder_output = model.encoder(input_ids, attention_mask)\n",
    "    decoder_output = model.decoder(decoder_input, encoder_output, attention_mask, decoder_mask)\n",
    "    # Kiểm tra kích thước đầu ra của decoder trước khi đưa vào ProjectionLayer\n",
    "    print(\"Decoder output size:\", decoder_output.size())  # Nên là (batch_size, seq_len, d_model)\n",
    "\n",
    "    # Sau đó truyền nó vào lớp ProjectionLayer\n",
    "    output = model.projection(decoder_output)\n",
    "\n",
    "    # Kiểm tra lại kích thước đầu ra sau ProjectionLayer\n",
    "    print(\"Final output size:\", output.size())  # Nên là (batch_size, seq_len, vocab_size)\n",
    "\n",
    "\n",
    "# In ra kích thước đầu ra và giá trị của đầu ra\n",
    "print(\"Output size:\", output.size())\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chaos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
